{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Sentiment Analysis - Gaussian Naive Bayes Implementation\n",
    "### Fatih KIYIKÇI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(['a', 'acaba', 'altı', 'ama', 'ancak', 'artık', 'asla', 'aslında', 'az', 'b', 'bana', 'bazen', 'bazı', 'bazıları', 'bazısı', 'belki', 'ben', 'beni', 'benim', 'beş', 'bile', 'bir', 'birçoğu', 'birçok', 'birçokları', 'biri', 'birisi', 'birkaç', 'birkaçı', 'birşey', 'birşeyi', 'biz', 'bize', 'bizi', 'bizim', 'böyle', 'böylece', 'bu', 'buna', 'bunda', 'bundan', 'bunu', 'bunun', 'burada', 'bütün', 'c', 'ç', 'çoğu', 'çoğuna', 'çoğunu', 'çok', 'çünkü', 'd', 'da', 'daha', 'de', 'değil', 'demek', 'diğer', 'diğeri', 'diğerleri', 'diye', 'dokuz', 'dolayı', 'dört', 'e', 'elbette', 'en', 'f', 'fakat', 'falan', 'felan', 'filan', 'g', 'gene', 'gibi', 'ğ', 'h', 'hâlâ', 'hangi', 'hangisi', 'hani', 'hatta', 'hem', 'henüz', 'hep', 'hepsi', 'hepsine', 'hepsini', 'her', 'her biri', 'herkes', 'herkese', 'herkesi', 'hiç', 'hiç kimse', 'hiçbiri', 'hiçbirine', 'hiçbirini', 'ı', 'i', 'için', 'içinde', 'iki', 'ile', 'ise', 'işte', 'j', 'k', 'kaç', 'kadar', 'kendi', 'kendine', 'kendini', 'ki', 'kim', 'kime', 'kimi', 'kimin', 'kimisi', 'l', 'm', 'madem', 'mı', 'mı', 'mi', 'mu', 'mu', 'mü', 'mü', 'n', 'nasıl', 'ne', 'ne kadar', 'ne zaman', 'neden', 'nedir', 'nerde', 'nerede', 'nereden', 'nereye', 'nesi', 'neyse', 'niçin', 'niye', 'o', 'on', 'ona', 'ondan', 'onlar', 'onlara', 'onlardan', 'onların', 'onların', 'onu', 'onun', 'orada', 'oysa', 'oysaki', 'ö', 'öbürü', 'ön', 'önce', 'ötürü', 'öyle', 'p', 'r', 'rağmen', 's', 'sana', 'sekiz', 'sen', 'senden', 'seni', 'senin', 'siz', 'sizden', 'size', 'sizi', 'sizin', 'son', 'sonra', 'ş', 'şayet', 'şey', 'şeyden', 'şeye', 'şeyi', 'şeyler', 'şimdi', 'şöyle', 'şu', 'şuna', 'şunda', 'şundan', 'şunlar', 'şunu', 'şunun', 't', 'tabi', 'tamam', 'tüm', 'tümü', 'u', 'ü', 'üç', 'üzere', 'v', 'var', 've', 'veya', 'veyahut', 'y', 'ya', 'ya da', 'yani', 'yedi', 'yerine', 'yine', 'yoksa', 'z', 'zaten', 'zira'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenizer(doc: str, stop_words=[]) -> str:\n",
    "    if stop_words:\n",
    "        #remove stopwords\n",
    "        filtered = [w for w in re.findall(r'\\w+', doc) if not w in stop_words and len(w) > 2] \n",
    "        doc = \" \".join(filtered)\n",
    "    # remove punctuation\n",
    "    remove_punct = str.maketrans('', '', string.punctuation+'–•’')\n",
    "    doc = doc.translate(remove_punct)\n",
    "    #lowercase the letters\n",
    "    doc = doc.lower()\n",
    "    #remove digits\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    doc = doc.translate(remove_digits)\n",
    "    #tokenize\n",
    "    filtered = [w for w in doc.split()]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "# tfidfVector = np.matrix(tfidfVector)\n",
    "\n",
    "class TfidfVectorizer():\n",
    "    def __init__(self, tokenizer, stopwords):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "\n",
    "    def getTF(self, doc: str, tokenize: bool) -> dict:\n",
    "        if tokenize:\n",
    "            if self.stopwords:\n",
    "                doc = self.tokenizer(doc,self.stopwords)\n",
    "            else:\n",
    "                doc = self.tokenizer(doc)\n",
    "        # Counts the number of appearances of every term.\n",
    "        counter = Counter(doc)\n",
    "\n",
    "        # Divide the number of appearances of every word to the length of the document(words)\n",
    "        for word in counter:\n",
    "            # counter holds tf now\n",
    "            counter[word] = counter[word] / len(counter)\n",
    "        return counter\n",
    "\n",
    "    def getIDF(self, TF: dict):\n",
    "        counts = defaultdict()\n",
    "        IDF = defaultdict()\n",
    "        for doc in TF.values():\n",
    "            for word in doc:\n",
    "                if word in counts:\n",
    "                    counts[word] += 1\n",
    "                else:\n",
    "                    counts[word] = 1\n",
    "        for word in counts:\n",
    "            IDF[word] = np.log((1 + len(corpus)) / (1 + counts[word])) + 1\n",
    "        # Create a list of unique words\n",
    "        self.words = counts\n",
    "        self.wordDict = sorted(counts.keys())\n",
    "        return IDF, counts\n",
    "\n",
    "    # tf-idf(word) -> tf(word) * idf(word)\n",
    "    def getTFIDF(self, tfDict: dict, idfDict: dict) -> dict:\n",
    "        docTFIDFDict = {}\n",
    "        for word in tfDict:\n",
    "            docTFIDFDict[word] = tfDict[word] * idfDict[word]\n",
    "        return docTFIDFDict\n",
    "\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        # tf-idf(word) -> tf(word) * idf(word)\n",
    "        TF = {}\n",
    "        for i, doc in enumerate(corpus):\n",
    "            TF[i] = self.getTF(doc, tokenize=True)\n",
    "        # countDict = calculateCountDict(tfDict)\n",
    "        IDF, counts = self.getIDF(TF)\n",
    "        self.tfidfDict = [self.getTFIDF(tf, IDF) for tf in TF.values()]\n",
    "        return self\n",
    "\n",
    "    def calculateTFIDFVector(self, doctfidf):\n",
    "        # Create an empty matrix to store the tfidf values\n",
    "        tfidfVector = [0.0] * len(self.wordDict)\n",
    "\n",
    "        # For each unique term, if it is in the document, store its TF-IDF value.\n",
    "        for i, word in enumerate(self.wordDict):\n",
    "            if word in doctfidf:\n",
    "                tfidfVector[i] = doctfidf[word]\n",
    "        return tfidfVector\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return [t for t, i in sorted(self.words.items(),\n",
    "                              key=itemgetter(1))]\n",
    "\n",
    "    def getMatrix(self):\n",
    "        self.tfidfMatrix = [self.calculateTFIDFVector(doctfidf) for doctfidf in self.tfidfDict]\n",
    "        return np.array(self.tfidfMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os\n",
    "import re as re\n",
    "import zipfile as zipfile\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "mytextzip = ''\n",
    "corpus = []\n",
    "labels = []\n",
    "def readfromzip(zipname: str, n_docs: int) -> None:\n",
    "    global mytextzip\n",
    "    global c\n",
    "    with zipfile.ZipFile(zipname) as z:\n",
    "        for zipinfo in z.infolist():\n",
    "            if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):\n",
    "                with z.open(zipinfo) as f:\n",
    "                    textfile = io.TextIOWrapper(f, encoding='cp1254', newline='\\r\\n')\n",
    "                    for line in textfile:\n",
    "                        if len(line):\n",
    "                            if re.search(r'([a-zA-Z]+\\r\\n)', line):\n",
    "                                mytextzip += line.strip() + ' '\n",
    "                                continue\n",
    "                            mytextzip += line.strip()\n",
    "                    corpus.append(mytextzip)\n",
    "                    labels.append(zipinfo.filename.split(\"/\")[2])\n",
    "                    if len(corpus) >= n_docs:\n",
    "                        break\n",
    "                    mytextzip =''\n",
    "readfromzip('film_yorumlari.zip',1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        subdata = self.seperate(X, y)\n",
    "\n",
    "        # find mean for every class\n",
    "        self.means= {k: np.mean(subdata[k], axis=0) for k in subdata.keys()}\n",
    "        # find var for every class, smooth the variance to avoid divide by zero. (sklearn uses the same method)\n",
    "        self.vars = {k: np.var(subdata[k], axis=0) + 1e-9 * np.var(X, axis = 0).max() for k in subdata.keys()}\n",
    "\n",
    "\n",
    "    def seperate(self, features, labels):\n",
    "        # seperate every class of feature in a dict\n",
    "        self.subdata = defaultdict(list)\n",
    "        self.classes = list(set(labels))\n",
    "        for idx in range(len(features)):\n",
    "            self.subdata[labels[idx]].append(features[idx])\n",
    "        length = len(labels)\n",
    "        #freq is the prior probability\n",
    "        self.priors = dict(map(lambda item: (item[0], (item[1]/length)),Counter(labels).items()))\n",
    "        return self.subdata\n",
    "\n",
    "    def prob(self, x, class_idx):\n",
    "        mean = self.means[class_idx]\n",
    "        var = self.vars[class_idx]\n",
    "        # numerator = np.exp(-(x - mean) ** 2 / (2 * std))\n",
    "        # denominator = np.sqrt(2 * np.pi * std)\n",
    "        # print(math.sqrt(2*np.pi*std))\n",
    "        # print(numerator, denominator)\n",
    "        # return numerator / denominator\n",
    "        sum_cp = - (0.5 * np.sum(((x - mean) ** 2) / var))\n",
    "        sum_cp += - (0.5 * np.sum(np.log(2 * np.pi * var)))\n",
    "        return sum_cp\n",
    "\n",
    "    def _pred(self, x):\n",
    "        # predict the class of the feature\n",
    "        posteriors = []\n",
    "        # calculate the probability of the feature for each class\n",
    "        for idx,c in enumerate(self.classes):\n",
    "            # prior probability of every class\n",
    "            prior = np.log(self.priors[c])\n",
    "            # sum of conditional probability of the feature vector\n",
    "            sum_cp = np.sum(self.prob(x, class_idx=c))\n",
    "            # total probability\n",
    "            posterior = prior + sum_cp\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def pred(self, X):\n",
    "        # call _pred for every feature vector\n",
    "        y_pred = [self._pred(x) for x in X]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, stopwords= stop_words)\n",
    "tfidf = tfidf.fit(corpus)\n",
    "tf_matrix = tfidf.getMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_matrix[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, test_size, set_seed):\n",
    "    arrays = [X,y]\n",
    "    assert all(len(arr) == len(arrays[0]) for arr in arrays)\n",
    "    seed = np.random.randint(0, 2**(32 - 1) - 1) if set_seed < 0 else set_seed\n",
    "    \n",
    "    for arr in arrays:\n",
    "        rstate = np.random.RandomState(seed)\n",
    "        rstate.shuffle(arr)\n",
    "        \n",
    "    size = int(len(X)*test_size)\n",
    "    x_train, x_test = X[size:,:], X[:size,:]\n",
    "    y_train, y_test = y[size:], y[:size]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test = train_test_split(tf_matrix, labels, test_size = 0.15, random_state = 23)\n",
    "X_train, X_test, y_train, y_test = split(tf_matrix, labels, test_size=0.15,set_seed=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.pred(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    acc = 0\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            acc += 1\n",
    "    return (acc*100)/len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Implementation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Implementation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
