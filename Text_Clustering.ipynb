{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Text Clustering - Kmeans Implementation\n",
    "### Fatih KIYIKÇI\n",
    "### 190709032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/fatihkykc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.add('it’s')\n",
    "\n",
    "def tokenizer(doc: str, stop_words=[]) -> str:\n",
    "    if stop_words:\n",
    "        #remove stopwords\n",
    "        filtered = [w for w in re.findall(r'\\w+', doc) if not w in stop_words and len(w) > 2] \n",
    "        doc = \" \".join(filtered)\n",
    "    # remove punctuation\n",
    "    remove_punct = str.maketrans('', '', string.punctuation+'–•’')\n",
    "    doc = doc.translate(remove_punct)\n",
    "    #lowercase the letters\n",
    "    doc = doc.lower()\n",
    "    #remove digits\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    doc = doc.translate(remove_digits)\n",
    "    #tokenize\n",
    "    filtered = [w for w in doc.split()]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "# tfidfVector = np.matrix(tfidfVector)\n",
    "\n",
    "class TfidfVectorizer():\n",
    "    def __init__(self, tokenizer, stopwords):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "\n",
    "    def getTF(self, doc: str, tokenize: bool) -> dict:\n",
    "        if tokenize:\n",
    "            if self.stopwords:\n",
    "                doc = self.tokenizer(doc,self.stopwords)\n",
    "            else:\n",
    "                doc = self.tokenizer(doc)\n",
    "        # Counts the number of appearances of every term.\n",
    "        counter = Counter(doc)\n",
    "\n",
    "        # Divide the number of appearances of every word to the length of the document(words)\n",
    "        for word in counter:\n",
    "            # counter holds tf now\n",
    "            counter[word] = counter[word] / len(counter)\n",
    "        return counter\n",
    "\n",
    "    def getIDF(self, TF: dict):\n",
    "        counts = defaultdict()\n",
    "        IDF = defaultdict()\n",
    "        for doc in TF.values():\n",
    "            for word in doc:\n",
    "                if word in counts:\n",
    "                    counts[word] += 1\n",
    "                else:\n",
    "                    counts[word] = 1\n",
    "        for word in counts:\n",
    "            IDF[word] = np.log((1 + len(corpus)) / (1 + counts[word])) + 1\n",
    "        # Create a list of unique words\n",
    "        self.words = counts\n",
    "        self.wordDict = sorted(counts.keys())\n",
    "        return IDF, counts\n",
    "\n",
    "    # tf-idf(word) -> tf(word) * idf(word)\n",
    "    def getTFIDF(self, tfDict: dict, idfDict: dict) -> dict:\n",
    "        docTFIDFDict = {}\n",
    "        for word in tfDict:\n",
    "            docTFIDFDict[word] = tfDict[word] * idfDict[word]\n",
    "        return docTFIDFDict\n",
    "\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        # tf-idf(word) -> tf(word) * idf(word)\n",
    "        TF = {}\n",
    "        for i, doc in enumerate(corpus):\n",
    "            TF[i] = self.getTF(doc, tokenize=True)\n",
    "        # countDict = calculateCountDict(tfDict)\n",
    "        IDF, counts = self.getIDF(TF)\n",
    "        self.tfidfDict = [self.getTFIDF(tf, IDF) for tf in TF.values()]\n",
    "        return self\n",
    "\n",
    "    def calculateTFIDFVector(self, doctfidf):\n",
    "        # Create an empty matrix to store the tfidf values\n",
    "        tfidfVector = [0.0] * len(self.wordDict)\n",
    "\n",
    "        # For each unique term, if it is in the document, store its TF-IDF value.\n",
    "        for i, word in enumerate(self.wordDict):\n",
    "            if word in doctfidf:\n",
    "                tfidfVector[i] = doctfidf[word]\n",
    "        return tfidfVector\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return [t for t, i in sorted(self.words.items(),\n",
    "                              key=itemgetter(1))]\n",
    "\n",
    "    def getMatrix(self):\n",
    "        self.tfidfMatrix = [self.calculateTFIDFVector(doctfidf) for doctfidf in self.tfidfDict]\n",
    "        return np.array(self.tfidfMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy,math\n",
    "\n",
    "def faster_cos(vec1, vec2):\n",
    "    return scipy.spatial.distance.cdist(vec1.reshape(1,-1), vec2.reshape(1,-1), 'cosine')\n",
    "\n",
    "# def dot_product(v1, v2):\n",
    "#     return sum(a * b for a, b in zip(v1, v2))\n",
    "\n",
    "\n",
    "# def magnitude(vector):\n",
    "#     return sqrt(dot_product(vector, vector))\n",
    "\n",
    "\n",
    "# def similarity(v1, v2):\n",
    "#     return dot_product(v1, v2) / (magnitude(v1) * magnitude(v2) + .00000000001)\n",
    "\n",
    "\n",
    "def cos_dist(vec1, vec2):\n",
    "    # cosine distance implementation, results are accurate,\n",
    "    # but very slow compared to scipy library, since every vector has 61k values.\n",
    "    # use \"faster_cos\" for large corpus\n",
    "    dot_product=np.dot(vec1,vec2)\n",
    "    mag1 = sum([x**2 for x in vec1])\n",
    "    mag2 = sum([x**2 for x in vec2])\n",
    "#     print(\"first {}, second  {}\".format(mag1,mag2))\n",
    "    magnitude_product = math.sqrt(mag1) * math.sqrt(mag2)\n",
    "    if not magnitude_product:\n",
    "        return 0.0\n",
    "    return 1 - float(dot_product) / magnitude_product\n",
    "\n",
    "class Kmeans:\n",
    "    def __init__(self, n_clusters, max_iters, tol=0.01):\n",
    "        self.k = n_clusters\n",
    "        self.tol = tol\n",
    "        self.max_iters = max_iters\n",
    "        self.centroids = []\n",
    "\n",
    "    def euc_distance(self, a, b):\n",
    "        # euclidian distance function, like cosine distance, it is slower than scipy.\n",
    "        # use Scipy for large corpus\n",
    "        start_time = time.time()\n",
    "        dist = [(a - b) ** 2 for a, b in zip(a, b)]\n",
    "        dist = math.sqrt(sum(dist))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"assigned in:\", elapsed_time)\n",
    "        return dist\n",
    "\n",
    "    def init_centroids(self):\n",
    "        self.centroidIndex = np.random.choice(self.m,self.k,replace=False)\n",
    "        self.centroids = self.data[self.centroidIndex]\n",
    "        return self.centroids\n",
    "\n",
    "    def assign_clusters(self, centroids):\n",
    "        start_time = time.time()\n",
    "        clusters = [[] for i in range(self.k)]\n",
    "        self.labels = []\n",
    "        for idx in range(len(self.data)):\n",
    "#             vec = vec.reshape(-1, 1)\n",
    "#             minDist = np.argmin([self.euc_distance(vec, centroids[c].reshape(-1,1)) for c in range(len(centroids))])\n",
    "            minDist = np.argmin([faster_cos(np.array(self.data[idx]), np.array(centroids[c])) for c in range(self.k)])\n",
    "            clusters[int(minDist)].append(idx)\n",
    "            self.labels.append(int(minDist))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"assigned in:\", elapsed_time)\n",
    "        return clusters\n",
    "\n",
    "    def update_centroids(self, clusters):\n",
    "        centroids = np.zeros((self.k, self.n))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.data[cluster], axis=0)\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        \n",
    "        # self.centroids = np.array([self.data[self.cluster_labels == i].mean(axis=0) for i in range(self.k)])\n",
    "        return centroids\n",
    "\n",
    "    def predict(self):\n",
    "        return self.assign_clusters()\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        self.m, self.n = self.data.shape\n",
    "        print(\"initilizing centroids\")\n",
    "        self.centroids = self.init_centroids()\n",
    "        for iter in range(self.max_iters):\n",
    "            print(\"assigning centroids\")\n",
    "            self.cluster_labels = self.assign_clusters(self.centroids)\n",
    "            self.old_centroids = self.centroids\n",
    "            print(\"updating centroids\")\n",
    "            self.centroids = self.update_centroids(self.cluster_labels)\n",
    "            if self.check_convergence(self.old_centroids, self.centroids):\n",
    "                break\n",
    "            print(\"Running iteration: \", iter)\n",
    "        return self\n",
    "\n",
    "    def check_convergence(self, old_centroids, centroids):\n",
    "        print(\"checking the convergence\")\n",
    "        distances = [faster_cos(np.array(centroids[c]),np.array(old_centroids[c])) for c in range(self.k)]\n",
    "        return sum(distances) < self.tol # tolerance value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(faster_cos(tf_matrix[0],tf_matrix[1]))\n",
    "# print(cos_dist(tf_matrix[0], tf_matrix[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "WfAaN6Ae3Jpg",
    "outputId": "ad96d482-e279-4c18-d059-df7498d2766b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io, os\n",
    "import re as re\n",
    "import zipfile as zipfile\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "mytextzip = ''\n",
    "corpus = []\n",
    "def readfromzip(zipname: str, n_docs: int) -> None:\n",
    "    global mytextzip\n",
    "    global c\n",
    "    with zipfile.ZipFile(zipname) as z:\n",
    "        for zipinfo in z.infolist():\n",
    "            if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):\n",
    "                with z.open(zipinfo) as f:\n",
    "                    textfile = io.TextIOWrapper(f, encoding='cp1254', newline='\\r\\n')\n",
    "                    for line in textfile:\n",
    "                        if len(line):\n",
    "                            if re.search(r'([a-zA-Z]+\\r\\n)', line):\n",
    "                                mytextzip += line.strip() + ' '\n",
    "                                continue\n",
    "                            mytextzip += line.strip()\n",
    "                    corpus.append(mytextzip)\n",
    "                    if len(corpus) >= n_docs:\n",
    "                        break\n",
    "                    mytextzip =''\n",
    "readfromzip('30Columnists.zip',1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, stopwords=stop_words)\n",
    "tfidf = tfidf.fit(corpus)\n",
    "tf_matrix = tfidf.getMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = Kmeans(n_clusters=15, max_iters=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initilizing centroids\n",
      "assigning centroids\n",
      "assigned in: 2.3849735260009766\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  0\n",
      "assigning centroids\n",
      "assigned in: 2.39237117767334\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  1\n",
      "assigning centroids\n",
      "assigned in: 2.348787546157837\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  2\n",
      "assigning centroids\n",
      "assigned in: 2.398411989212036\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  3\n",
      "assigning centroids\n",
      "assigned in: 2.408198118209839\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  4\n",
      "assigning centroids\n",
      "assigned in: 2.330413579940796\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  5\n",
      "assigning centroids\n",
      "assigned in: 2.3431079387664795\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  6\n",
      "assigning centroids\n",
      "assigned in: 2.329502820968628\n",
      "updating centroids\n",
      "checking the convergence\n",
      "Running iteration:  7\n",
      "assigning centroids\n",
      "assigned in: 2.343080997467041\n",
      "updating centroids\n",
      "checking the convergence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Kmeans at 0x7f08d88d9ba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "k_means.fit(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = {}\n",
    "# for i,cluster in enumerate(k_means.labels):\n",
    "#     doc = corpus[i]\n",
    "#     if cluster not in text.keys():\n",
    "#         text[cluster] = doc\n",
    "#     else:\n",
    "#         text[cluster] += doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = {}\n",
    "# counts = {}\n",
    "# for cluster in range(k_means.k):\n",
    "# #     word_sent = word_tokenize(text[cluster].lower())\n",
    "# #     word_sent = [word for word in word_sent if word not in stop_words]\n",
    "#     word_sent = tokenizer(text[cluster], stop_words= stop_words)\n",
    "#     freq = FreqDist(word_sent)\n",
    "#     keywords[cluster] = nlargest(100,freq, key=freq.get)\n",
    "#     counts[cluster]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_keys={}\n",
    "# for cluster in range(k_means.k):\n",
    "#     other_clusters = list(set(range(k_means.k))-set([cluster]))\n",
    "#     keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "#     unique=set(keywords[cluster])-keys_other_clusters\n",
    "#     unique_keys[cluster]=nlargest(10,unique, key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatihkykc/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words=stop_words)\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=15, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=15)\n",
    "km.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the most frequent words from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterWords(labels, cluster_centers,heuristic: bool):\n",
    "    text = {}\n",
    "    if not heuristic:\n",
    "        cluster_centers = len(cluster_centers)\n",
    "    for i,cluster in enumerate(labels):\n",
    "        doc = corpus[i]\n",
    "        if cluster not in text.keys():\n",
    "            text[cluster] = doc\n",
    "        else:\n",
    "            text[cluster] += doc\n",
    "    keywords = {}\n",
    "    counts = {}\n",
    "    for cluster in range(cluster_centers):\n",
    "        word_sent = tokenizer(text[cluster], stop_words= stop_words)\n",
    "        freq = FreqDist(word_sent)\n",
    "        keywords[cluster] = nlargest(100,freq, key=freq.get)\n",
    "        counts[cluster]=freq\n",
    "    unique_keys={}\n",
    "    for cluster in range(cluster_centers):\n",
    "        other_clusters = list(set(range(cluster_centers))-set([cluster]))\n",
    "        try:\n",
    "            keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "        except:\n",
    "            keys_other_clusters=set(keywords[other_clusters[0]])\n",
    "        unique=set(keywords[cluster])-keys_other_clusters\n",
    "        unique_keys[cluster]=nlargest(10,unique, key=counts[cluster].get)\n",
    "    return unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearnRes = clusterWords(km.labels_, km.cluster_centers_,heuristic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeuristicRes = clusterWords(k_means.labels, k_means.k, heuristic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['snp',\n",
       "  'labour',\n",
       "  'government',\n",
       "  'elections',\n",
       "  'scottish',\n",
       "  'says',\n",
       "  'tax',\n",
       "  'election',\n",
       "  'council',\n",
       "  'parliament'],\n",
       " 1: ['study',\n",
       "  'risk',\n",
       "  'patients',\n",
       "  'women',\n",
       "  'disease',\n",
       "  'found',\n",
       "  'heart',\n",
       "  'studies',\n",
       "  'blood',\n",
       "  'health'],\n",
       " 2: ['league',\n",
       "  'football',\n",
       "  'club',\n",
       "  'united',\n",
       "  'west',\n",
       "  'fans',\n",
       "  'ham',\n",
       "  'premier',\n",
       "  'season',\n",
       "  'cup'],\n",
       " 3: ['bank',\n",
       "  'bn',\n",
       "  'barclays',\n",
       "  'banking',\n",
       "  'hbos',\n",
       "  'capital',\n",
       "  'banks',\n",
       "  'shares',\n",
       "  'lloyds',\n",
       "  'investors'],\n",
       " 4: ['independence',\n",
       "  'powers',\n",
       "  'brown',\n",
       "  'referendum',\n",
       "  'power',\n",
       "  'policy',\n",
       "  'devolution',\n",
       "  'gordon',\n",
       "  'scots',\n",
       "  'glasgow'],\n",
       " 5: ['insurance',\n",
       "  'plans',\n",
       "  'plan',\n",
       "  'pay',\n",
       "  'company',\n",
       "  'coverage',\n",
       "  'medicare',\n",
       "  'companies',\n",
       "  'doctor',\n",
       "  'cost'],\n",
       " 6: ['obama',\n",
       "  'mccain',\n",
       "  'president',\n",
       "  'barack',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'state',\n",
       "  'campaign',\n",
       "  'world',\n",
       "  'senate'],\n",
       " 7: ['brown',\n",
       "  'gordon',\n",
       "  'prime',\n",
       "  'cameron',\n",
       "  'david',\n",
       "  'tory',\n",
       "  'that',\n",
       "  'blair',\n",
       "  'cabinet',\n",
       "  'britain'],\n",
       " 8: ['group',\n",
       "  'sales',\n",
       "  'christmas',\n",
       "  'profits',\n",
       "  'shares',\n",
       "  'strong',\n",
       "  'retail',\n",
       "  'business',\n",
       "  'bn',\n",
       "  'today'],\n",
       " 9: ['economic',\n",
       "  'bank',\n",
       "  'financial',\n",
       "  'economy',\n",
       "  'inflation',\n",
       "  'recession',\n",
       "  'policy',\n",
       "  'world',\n",
       "  'crisis',\n",
       "  's'],\n",
       " 10: ['breast',\n",
       "  'prostate',\n",
       "  'surgery',\n",
       "  'doctor',\n",
       "  'hpv',\n",
       "  'screening',\n",
       "  'cancers',\n",
       "  'lung',\n",
       "  'colon',\n",
       "  'therapy'],\n",
       " 11: ['team',\n",
       "  'league',\n",
       "  'players',\n",
       "  'game',\n",
       "  'season',\n",
       "  'united',\n",
       "  'england',\n",
       "  'world',\n",
       "  'baseball',\n",
       "  'football'],\n",
       " 12: ['world',\n",
       "  'work',\n",
       "  'made',\n",
       "  'might',\n",
       "  'brown',\n",
       "  'business',\n",
       "  'pay',\n",
       "  'yet',\n",
       "  'see',\n",
       "  'since'],\n",
       " 13: ['bank',\n",
       "  'banks',\n",
       "  'financial',\n",
       "  'market',\n",
       "  'banking',\n",
       "  'management',\n",
       "  'bn',\n",
       "  'companies',\n",
       "  'business',\n",
       "  'markets'],\n",
       " 14: ['kids',\n",
       "  'fitness',\n",
       "  'workout',\n",
       "  'school',\n",
       "  'muscles',\n",
       "  'training',\n",
       "  'you',\n",
       "  'fun',\n",
       "  'child',\n",
       "  'music']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearnRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['united',\n",
       "  'liverpool',\n",
       "  'chelsea',\n",
       "  'league',\n",
       "  'team',\n",
       "  'season',\n",
       "  'ferguson',\n",
       "  'last',\n",
       "  'manchester',\n",
       "  'mourinho'],\n",
       " 1: ['kids',\n",
       "  'family',\n",
       "  'school',\n",
       "  'child',\n",
       "  'fitness',\n",
       "  'parents',\n",
       "  'workout',\n",
       "  'often',\n",
       "  'muscles',\n",
       "  'training'],\n",
       " 2: ['study',\n",
       "  'risk',\n",
       "  'disease',\n",
       "  'blood',\n",
       "  'studies',\n",
       "  'found',\n",
       "  'vitamin',\n",
       "  'brain',\n",
       "  'researchers',\n",
       "  'high'],\n",
       " 3: ['alcohol',\n",
       "  'drinking',\n",
       "  'drink',\n",
       "  'women',\n",
       "  'found',\n",
       "  'per',\n",
       "  'blood',\n",
       "  'study',\n",
       "  'pubs',\n",
       "  'cent'],\n",
       " 4: ['obama',\n",
       "  'new',\n",
       "  'and',\n",
       "  'president',\n",
       "  'mccain',\n",
       "  'public',\n",
       "  'state',\n",
       "  'say',\n",
       "  'management',\n",
       "  'house'],\n",
       " 5: ['baseball',\n",
       "  'new',\n",
       "  'series',\n",
       "  'games',\n",
       "  'mets',\n",
       "  'york',\n",
       "  'yankees',\n",
       "  'sox',\n",
       "  'run',\n",
       "  'since'],\n",
       " 6: ['patients',\n",
       "  'medicare',\n",
       "  'drugs',\n",
       "  'drug',\n",
       "  'plan',\n",
       "  'treatment',\n",
       "  'plans',\n",
       "  'part',\n",
       "  'effects',\n",
       "  'care'],\n",
       " 7: ['city',\n",
       "  'newcastle',\n",
       "  'new',\n",
       "  'kinnear',\n",
       "  'hughes',\n",
       "  'and',\n",
       "  'ashley',\n",
       "  'million',\n",
       "  'joey',\n",
       "  'robinho'],\n",
       " 8: ['bank',\n",
       "  'government',\n",
       "  'banks',\n",
       "  'financial',\n",
       "  'bn',\n",
       "  'banking',\n",
       "  'market',\n",
       "  'new',\n",
       "  'economic',\n",
       "  'per'],\n",
       " 9: ['england',\n",
       "  'johnson',\n",
       "  'rugby',\n",
       "  'coach',\n",
       "  'squad',\n",
       "  'six',\n",
       "  'english',\n",
       "  'france',\n",
       "  'new',\n",
       "  'games'],\n",
       " 10: ['labour',\n",
       "  'brown',\n",
       "  'government',\n",
       "  'party',\n",
       "  'minister',\n",
       "  'scotland',\n",
       "  'scottish',\n",
       "  'snp',\n",
       "  'gordon',\n",
       "  'new'],\n",
       " 11: ['wenger',\n",
       "  'clubs',\n",
       "  'and',\n",
       "  'west',\n",
       "  'english',\n",
       "  'spurs',\n",
       "  'redknapp',\n",
       "  'top',\n",
       "  'ham',\n",
       "  'new'],\n",
       " 12: ['cent',\n",
       "  'per',\n",
       "  'public',\n",
       "  'mps',\n",
       "  'brown',\n",
       "  'points',\n",
       "  'labour',\n",
       "  'lords',\n",
       "  'since',\n",
       "  'new'],\n",
       " 13: ['cancer',\n",
       "  'women',\n",
       "  'breast',\n",
       "  'risk',\n",
       "  'men',\n",
       "  'patients',\n",
       "  'prostate',\n",
       "  'disease',\n",
       "  'study',\n",
       "  'treatment'],\n",
       " 14: ['insurance',\n",
       "  'care',\n",
       "  'medical',\n",
       "  'company',\n",
       "  'pay',\n",
       "  'doctor',\n",
       "  'patients',\n",
       "  'plans',\n",
       "  'companies',\n",
       "  'physician']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HeuristicRes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "L39jrAuH2-j-",
    "ACwXfVcs3By_"
   ],
   "name": "CENG3526-NLP-HW-Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
